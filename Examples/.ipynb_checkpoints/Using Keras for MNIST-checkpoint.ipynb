{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184d0ca4",
   "metadata": {},
   "source": [
    "The easiest way to get your hands dirty and do some practical work is using Keras. Keras - I still remember - used to be the _de facto_ library for DL back in 2016-18. Gradually, with retirement of Theano and CNTK, it reduced to a mere TensorFlow wrapper. Also, the rise of PyTorch and JAX meant Keras fell out of the favour.\n",
    "\n",
    "Recently, Keras developers realized the need of the hour and introduced [Keras Core](https://keras.io/keras_core/), which supports both PyTorch and JAX. Its expected to be released as Keras 3.0 soon. Let's get started.\n",
    "\n",
    "We will use JAX here as a backend, so specifying it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072599e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set backend env to JAX\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd14866e",
   "metadata": {},
   "source": [
    "> **Note:** Specifying the backend after importing Keras will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324f302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 14:20:00.184429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras_core as kr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cdff46",
   "metadata": {},
   "source": [
    "Now we will take one of the most commonly used/basic dataset: MNIST. MNIST is a dataset comprising of a number of scanned hand-written digits (from 0-9). This dataset is already available in Keras datasets.\n",
    "\n",
    "Its already divided among training and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b0d65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xTrain, yTrain), (xTest, yTest) = kr.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d953bb57",
   "metadata": {},
   "source": [
    "In MNIST, each image is of dimension $28 \\times 28$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95f513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10\n",
    "imageShape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec521a",
   "metadata": {},
   "source": [
    "Usually, for the multiclass classification problem (like ours), we don't use direct labels of 0,1,2,... but instead use **One-hot encoding** which converts each label into a vector of length $c$ (where $c$ is number of classes we have). This vector has all the entries zero except the $j^{th}$ entry, where $j$ is its respective class (like 7 for the digit 7).\n",
    "\n",
    "It can be achieved in Keras using **`to_categorical()`**. Easy peasy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d3a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_core.utils import to_categorical\n",
    "\n",
    "yTrain = to_categorical(yTrain, C)\n",
    "yTest = to_categorical(yTest, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa35ab",
   "metadata": {},
   "source": [
    "We will specify the hyperparameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8323289",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 128\n",
    "numberEpochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e4c35",
   "metadata": {},
   "source": [
    "Next step is to define the neural network (CNN in this case)'s model. Here we can see the beauty/ease of use of Keras as making layers of a neural network in Keras is as smooth as silk. For that, we can import `layers` from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd03ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_core import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cf273",
   "metadata": {},
   "source": [
    "## Sequential API\n",
    "\n",
    "Keras has two APIs: One of them is sequential. Sequential is pretty straightforward and allows us to make a neural network by simply stacking the layers on the top of each other with respective parameters. The output of each layer becomes the input of the succeeding one.\n",
    "\n",
    "It has some pretty basic functions, for example:\n",
    "\n",
    "### Input layer\n",
    "\n",
    "The `Input()` as its name depicts is used to define the input layer. It takes the input (be it an image or any type of data)'s dimensions as an input.\n",
    "\n",
    "**Caution:** Don't pass the input image etc itself as an input here. That's a later on job at the time of optimization. Right now, we are just defining the model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e279d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayer = layers.Input(shape=imageShape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943adae",
   "metadata": {},
   "source": [
    "### Convolution layer\n",
    "\n",
    "**`Conv2D()`** is a quite import function used to define the convolutional layer. Its arguments are:\n",
    "\n",
    "- **Number of filters:** In order to ensure we don't overfit (or underfit in some cases) to a single filter, we can define a number of filters. Each filter has the same size, but is applied (and learns) independently to each other.\n",
    "- **`kernel_size`:** Usually, we define it as an odd number (you are free to define any size of filter as you would like to) like $3 \\times 3$, $5 \\times 5$, etc. Here, MNIST images are already pretty small, so $3 \\times 3$ will work.\n",
    "- **`activation`:** The activation function to use. Usually, we use ReLU for the intermediate layers. Please feel free to try others too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff0659c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "convLayer = layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeae6b4",
   "metadata": {},
   "source": [
    "### Pooling layer\n",
    "\n",
    "Since convolution involves backpropagation and a number of parameters, so we can simply use a pooling filter to reduce the dimensions of an image. It simply takes the max or average (we have support for both) of the pixels under the filter. Its only argument is **`pool_size`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c2aba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "poolLayer = layers.MaxPooling2D(pool_size=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3d59b",
   "metadata": {},
   "source": [
    "That completes our first layer. Usually, a CNN always comprises of a convolution layer, followed by a pooling layer. So let's rename the above layers to add the 1 as index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "931c215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convLayer1 = convLayer\n",
    "poolLayer1 = poolLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924ac2a",
   "metadata": {},
   "source": [
    "And we can add another layer too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a615f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "convLayer2 = layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")\n",
    "poolLayer2 = layers.AveragePooling2D(pool_size=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd7fe75",
   "metadata": {},
   "source": [
    "Finally, we can simply flatten the output of this last layer, so we can apply a normal neural network layer on it in the end. We can simply do it using `Flatten()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc4bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = layers.Flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762cef22",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "There are a number of phenomenons to avoid overfitting and one of them is Dropout. The intuition is pretty straightforward: Make sure that your model doesn't rely on some specific neurons too much and hence don't use some f them (picked randomly) in each iteration.\n",
    "\n",
    "For Dropout, we simply call the function mentioning the ratio of neurons to be dropped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e1e8726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "dropOut = layers.Dropout(0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97070af6",
   "metadata": {},
   "source": [
    "### Dense\n",
    "\n",
    "For normal feedforward neural networks, we use **`Dense()`** layer. It takes the number of classes, followed by the activation function. \n",
    "\n",
    "**Suggestion:** Use Softmax for multiclass and Sigmoid/Tanh for the binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9be4bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputLayer = layers.Dense(C, activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80e2e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel = kr.Sequential(\n",
    "    [\n",
    "        inputLayer,\n",
    "        convLayer1,\n",
    "        poolLayer1,\n",
    "        convLayer2,\n",
    "        poolLayer2,\n",
    "        flatten,\n",
    "        dropOut,\n",
    "        outputLayer,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7535907",
   "metadata": {},
   "source": [
    "It would be quite useful to check how model is shrinking the image and the number of parameters. For that, we can simple call `<model name>.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74cb5a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ average_pooling2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,010</span> │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ average_pooling2d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)              │          \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)              │          \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                │     \u001b[38;5;34m16,010\u001b[0m │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,826\u001b[0m (136.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,826\u001b[0m (136.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnnModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512e724",
   "metadata": {},
   "source": [
    "Having specified both hyperparameters and parameters, we can optimize/train the model. But wait a min, we need to specify the loss function, our optimizer and the final metric for evaluation as well. **`compile()`** is there to serve the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "134c64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel.compile(\n",
    "    loss=\"mean_squared_error\", \n",
    "    optimizer=\"sgd\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a802cbb",
   "metadata": {},
   "source": [
    "Please feel free to play around with the model's definition above to use different loss functions or optimizers like Adam, etc. We can even try different evaluation metrices too, though I would recommend to go there a bit later.\n",
    "\n",
    "Since a convolution operator assumes an input to have height, width and number of channels, we will have to convert our inputs accordingly. We don't have any channel for these black & white images, but can simply add one using NumPy's [expand_dims()](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5637c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = np.expand_dims(xTrain, -1)\n",
    "xTest = np.expand_dims(xTest, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd4a12",
   "metadata": {},
   "source": [
    "Finally, we can train it. Since the basic objective of each ML model is to not only optimize it well (low training error) but also generalize well (low test error). Often we don't have enough testing data, so we can approximate the test accuracy/error by **validation**. It is performed by dividing the traininig subset further into training and validation.\n",
    "\n",
    "Keras (and other ML/DL libraries) allow us the luxury of validating the model during the training. In Keras, all we have to do is to specify the `validation_split`. 0.1 means 10% and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6d13dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 80ms/step - accuracy: 0.2621 - loss: 0.1411 - val_accuracy: 0.5797 - val_loss: 0.0768\n",
      "Epoch 2/3\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 83ms/step - accuracy: 0.5383 - loss: 0.0852 - val_accuracy: 0.7388 - val_loss: 0.0469\n",
      "Epoch 3/3\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 80ms/step - accuracy: 0.6664 - loss: 0.0591 - val_accuracy: 0.8155 - val_loss: 0.0294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_core.src.callbacks.history.History at 0x7fae706ad190>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnModel.fit(\n",
    "    xTrain, yTrain, batch_size=batchSize, epochs=numberEpochs, validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "057efc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7960 - loss: 0.0327\n",
      "Test loss: 0.031506579369306564\n",
      "Test accuracy: 0.8059999942779541\n"
     ]
    }
   ],
   "source": [
    "score = cnnModel.evaluate(xTest, yTest, verbose=1)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f613a3",
   "metadata": {},
   "source": [
    "That's it from our side. But there's much more to do here. Please use it as a launching pad to explore more avenues. Curiosity doesn't have (**shouldn't have**) any limits. Carpe diem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
