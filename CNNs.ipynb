{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO910emuJ80hw+ALbaOoZQ4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNNs\n",
        "\n",
        "Feedforward (or fully connected) networks lead to a number of parameters. Inspired by the human (or cat's in fact) visual cortex, convolutional neural networks (CNNs) are primarily used for the Computer Vision tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "1FsZK-6aEqg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution Operation\n",
        "\n",
        "Convolution operation is inspired from Digital Signal Processing (its ok if you don't know about it), where we convolve a filter with the input signal.\n",
        "\n",
        "Luckily, its much easier to understand it for images in 2D.\n",
        "\n",
        "![](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif)\n",
        "\n"
      ],
      "metadata": {
        "id": "iz8pVaZnFS0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a number of ways of performing the convolution. Regardless of the type, in convolution, we have:\n",
        "\n",
        "- Input image ($I$) having dimensions $m \\times n$.\n",
        "- Convolution filter ($F$) having dimensions $f \\times f$. Not only they are square, but usually its common to have them in an odd order: $3 \\times 3, 5 \\times 5$, etc.\n",
        "- And output/resultant image ($O$) having dimensions $x \\times y$. We will look more into it soon.\n",
        "\n",
        "For applying convolution, we place the filter on the top left and calculate the weighted sum of the underlying (input image's) pixels as:\n",
        "\n",
        "$$s = \\sum_{i,j=0}^{f-1}  I_{p,q}F_{i,j}$$\n",
        "\n",
        "Dont be worried by the above equation (yeah, its not perfectly written). It just means that we take the product of first pixel with the filter's first pixel, second's with the filter's second pixel and so on.\n",
        "\n",
        "As an example: We have a filter matrix as:\n",
        "\n",
        "<TODO: insert matrices for both filter and image>\n",
        "\n"
      ],
      "metadata": {
        "id": "aLjG1wpBMLtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output image has smaller resolution\n",
        "\n",
        "As we can see in the gif above, we can't convolve beyond the last column, which means that output image will be _slimmer_ than the input one. And similarly, we can't convolve beyond the last row, which means it will be _shorter_ as well.\n",
        "\n",
        "If we look closely, the output will have 2 columns less than the input one. Same for the rows as well.\n",
        "\n",
        "In other words:\n",
        "\n",
        "$$x = m - f +1$$\n",
        "\n",
        "And similarly,\n",
        "\n",
        "$$y = n -f +1$$\n",
        "\n",
        "Collectively,\n",
        "\n",
        "$$(x,y) = (m-f+1,n-f+1)$$"
      ],
      "metadata": {
        "id": "4HHysIxzU1mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example\n",
        "\n",
        "We can better understand it with an example:\n",
        "\n",
        "Suppose, we have an image of $28 \\times 28$ (normal MNIST images) and want to apply $3 \\times 3$ filter on it. Resultant image's dimensions would be:\n",
        "\n",
        "$$(x,y) = (28-3+1,28-3+1)$$\n",
        "\n",
        "$$ = (26,26)$$\n",
        "\n",
        "Lets verify it in the code:"
      ],
      "metadata": {
        "id": "ts-hFb35Vxjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "jax.config.update('jax_platform_name','cpu')"
      ],
      "metadata": {
        "id": "Nr-yiu3eariD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can simply declare the image and filter as the JAX arrays."
      ],
      "metadata": {
        "id": "zbBuSxoJbHPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "I = jnp.ones((28,28))\n",
        "F = jnp.ones((3,3))"
      ],
      "metadata": {
        "id": "NK6Rion_aXW_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a number of ways of taking convolutions in JAX. I prefer the LAX method (Personally, I prefer PyTorch for this), `conv_general_dilated`.\n",
        "\n",
        "Its syntax is:\n",
        "\n",
        "`<output> = conv_general_dilated(<input images>,<filter>,....)`"
      ],
      "metadata": {
        "id": "wf5k_nPfbauB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.lax as lax\n",
        "O = lax.conv_general_dilated(I[None, None], F[None, None], (1,1),[(0,0),(0,0)])\n",
        "(O.shape[2],O.shape[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSowb-iwav5d",
        "outputId": "ba81e860-1eb9-415e-b9a7-c6e8065d691e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did I use `[None,None]` above or took only the last 2 parts of the `O`'s shape? Lets delay it for the time being.\n",
        "\n",
        "What is in the dots? We are going to study that now:"
      ],
      "metadata": {
        "id": "Xi9G22ZocCfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding\n",
        "\n",
        "Often there are cases where we need to add the padding with the input image to ensure that filter is applied throughout (and hence less or no shrinkage in size).\n",
        "\n",
        "![](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/arbitrary_padding_no_strides.gif)"
      ],
      "metadata": {
        "id": "y6kKpPCIXf7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In such a case, resultant image will be _stretched_ from both left and right (for width) and top and bottom (for height).\n",
        "\n",
        "If we have a padding of $1$ on each side, it means we will have an addition of $2$ in $x$ and $2$ in $y$ as well. Generalizing it for $p$ size of padding, we will get:\n",
        "\n",
        "$$(x,y) = (m-f+2p+1,n-f+2p+1)$$"
      ],
      "metadata": {
        "id": "EAl18nC_YFNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example\n",
        "\n",
        "Applying a padding of 2 for the above example, we get:\n",
        "\n",
        "$$(x,y) = (28-3+2(2)+1,28-3+2(2)+1)$$\n",
        "\n",
        "$$ = (30,30)$$\n",
        "\n",
        "Whoa! The output is bigger than the input. Rare, but theoretically it's still possible. We can confirm that too.\n",
        "\n",
        "Since all of the settings are same as the above example. We just need to call the **`conv_general_dilated()`** again.\n",
        "\n",
        "I just skipped its last two arguments. I will still skip the first one for a few mins, but the second one shows the padding. Lets set it accordingly and verify the effect on the output:"
      ],
      "metadata": {
        "id": "GSFxi3UCYweL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "O = lax.conv_general_dilated(I[None, None], F[None, None], (1,1),[(2,2),(2,2)])\n",
        "(O.shape[2],O.shape[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbVOVJS2ckZz",
        "outputId": "6755fd23-1933-4338-ad38-616ac43a9ff8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strides\n",
        "\n",
        "There can be scenarios where we have low computational resources and bigger images. In such cases, its useful to shrink the size considerably. For that, we can choose to filter by taking a jump or a stride rather than applying it consecutively.\n",
        "\n",
        "![](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif)\n"
      ],
      "metadata": {
        "id": "nJc4HMy-ZHoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In such a case(s), size decreases by a factor. If we take the stride of 2 (like in the example above), it will shrink the output by half. Similarly, if we do it by 3, it will be one-third and so on. Mathematically,\n",
        "\n",
        "$$(x,y) = (\\frac{m-f}{s}+1,\\frac{n-f}{s}+1)$$\n",
        "\n",
        "**Note:** Please note that stride is applied in both horizontal and vertical traversing.\n",
        "\n",
        ">It is the 3rd argument in the convolution function we have been using."
      ],
      "metadata": {
        "id": "pSSDmiZ3ZqeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Strides with Padding\n",
        "\n",
        "We can make it insane by having both strides and padding combined (doesn't make any sense to me, though).\n",
        "\n",
        "![](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif)\n",
        "\n",
        "In this scenario, we will have full equation:\n",
        "\n",
        "$$(x,y) = (\\frac{m-f+2p}{s}+1,\\frac{n-f+2p}{s}+1)$$\n",
        "\n",
        "Lets round it off by last example in which we are using the same padding ($2\\times2$) as above with stride of $2$:\n",
        "\n",
        "$$(x,y) = (\\frac{28-3+2(2)}{2}+1,\\frac{28-3+2(2)}{2}+1)$$\n",
        "\n",
        "$$=(15,15)$$"
      ],
      "metadata": {
        "id": "FTD1qNzLm-s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "O = lax.conv_general_dilated(I[None, None], F[None, None], (2,2),[(2,2),(2,2)])\n",
        "(O.shape[2],O.shape[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULKFOk9pduOZ",
        "outputId": "b9a80eae-af14-4a1a-a633-85933c0a3374"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling\n",
        "\n",
        "Usually pixels in a local region are pretty much similar (even more true for the bigger images), hence we can even simplify the scenario by just summing them up without any weights. In this case, our filter is a fixed function and doesn't need to learn any weights; hence a relaxer backprop.\n",
        "\n",
        "We can have these pooling operations:\n",
        "\n",
        "- Average\n",
        "- Max – Pick the maximum of the pixels under the filter.\n",
        "\n",
        "If you can think of any other pooling operator, feel free to try one. Actually, it would be a good practice.\n",
        "\n",
        "**Note:** Pooling is usually applied in a non-overlapping way. JAX allows us to set the strides manually too using the `strides` attribute.\n",
        "\n",
        "---\n",
        "\n",
        "<To be continued>"
      ],
      "metadata": {
        "id": "7q1s7RVHnknq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation\n",
        "\n",
        "JAX doesn't have an implementation (though its not tough to make it ourselves). PyTorch, Keras and Tensorflow provide it though."
      ],
      "metadata": {
        "id": "2qC1NZgcfyUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution with Channels\n",
        "\n",
        "If we recall, a colour image is 2D but it has 3 channels (RGB) as well, meaning our input is overall a 3D tensor. That'swhy we had two other dimensions other than the Height and the Width.\n",
        "\n",
        "What about the other dimension we skipped above? Its answer will be found in the starting lectures of ML (when instructors switch from GD to SGD and finally to Batched GD). Its pretty rare to use any of the extremes of SGD or GD, hence we input the data (including the images) in a batched form, making it a 4D combination (i.e, a tuple).\n",
        "\n",
        "4D can be hard to visualize for us, but for computers (and NumPy/JAX/PyTorch in particular) is as simple as normal 2D stuff.\n",
        "\n",
        "So finally, our input would be:\n",
        "\n",
        "$$N \\times C \\times H \\times W$$\n",
        "\n",
        "Where, $N$ is number of images in the input batch, $C$ is the number of channels and $H, W$ are height and width, respectively.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LEAcufGHmW7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does Convolution Work with Channels\n",
        "\n",
        "For multiple channels, convolution works pretty much the same way you are applying it normally for 2D (as we have had so far).\n",
        "\n",
        "The only difference would be that.. Umm let me explain it with an example.\n",
        "\n",
        "Lets suppose we have 3 channels (simply RGB). Usually, the number of filters is equal to the number of channels, so **we have 3 channels and 3 filters**.\n",
        "\n",
        "Now, we will take all the 3 channels separately and apply each filter on the respective channel. Obviously, each channel-filter pair will have a corresponding output.\n",
        "\n",
        "Let's see it using the nice animation (Credits: **[CS231n, Stanford](https://cs231n.github.io/convolutional-networks/#norm)**)."
      ],
      "metadata": {
        "id": "WK-1cHQCpe_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code credits: ChatGPT\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "iframe_code = f'<iframe src=\"https://cs231n.github.io/assets/conv-demo/index.html\" width=\"800\" height=\"700\"></iframe>'\n",
        "display(HTML(iframe_code))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "id": "3N89N4dTpPpU",
        "outputId": "eba3c83b-f2fe-4a54-bddd-f59302ddf633"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe src=\"https://cs231n.github.io/assets/conv-demo/index.html\" width=\"800\" height=\"700\"></iframe>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"fig figcenter fighighlight\">\n",
        "  \n",
        "  <iframe src=\"https://cs231n.github.io/assets/conv-demo/index.html\" width=\"100%\" height=\"400px;\" style=\"border:none;\"></iframe>\n",
        "  \n",
        "  <div class=\"figcaption\"></div>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "9XJ36TGGoelv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Implementation\n",
        "\n",
        "We can implement convolution in PyTorch as **`nn.Conv2D()`**."
      ],
      "metadata": {
        "id": "VRaj56p7gBH5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SoOwFhEGEham",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e117cfd5-299e-46b0-d315-c5a1a4a1a07e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "A = nn.Conv2d(3,3,3)\n",
        "A"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input = torch.ones((1,3,28,28))\n",
        "\n",
        "B = A(input)\n",
        "B.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J81VgiXDlYIK",
        "outputId": "63e520cd-6b43-4832-b6a5-eae916e235a2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 26, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the first time I am directly running a code without explaining it. Now, let me get back to it. PyTorch's function takes the following arguments:\n",
        "\n",
        "- Number of input channels\n",
        "- Number of output image's channels too\n",
        "- Filter Size – provide a normal number `n` and it will convert it into a `(n,n)` tuple automatically (like it done above).\n",
        "- And other arguments like stride or pooling.\n",
        "\n",
        "Please feel free to check it further."
      ],
      "metadata": {
        "id": "RGbrn477rTl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Note:** Technically, its not convolution, but is correlation instead."
      ],
      "metadata": {
        "id": "ioegdJ-3kPjd"
      }
    }
  ]
}