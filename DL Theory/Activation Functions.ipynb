{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba1460c",
   "metadata": {},
   "source": [
    "Recently, I got a chance to quiz some ML students and surprisingly, majority were clueless when asked, \"What is an activation function, and why it is used?\" They knew about ReLU, Sigmoid, etc. but were unable to identify why these functions are used. It led me to write a dedicated entry on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e953d397",
   "metadata": {},
   "source": [
    "## Perceptrons and MLPs\n",
    "\n",
    "Lets consider an MLP having an input, output and a couple of hidden layers (each having 3 neurons). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ed03f",
   "metadata": {},
   "source": [
    "Their equations will be:\n",
    "    \n",
    "$$a_1(1) = w_{11} (1) x_1 + w_{21} (1) x_2 + w_{31} (1) x_3$$\n",
    "\n",
    "$$a_2(1) = w_{12} (1) x_1 + w_{22} (1) x_2 + w_{32} (1) x_3$$\n",
    "\n",
    "And so on.\n",
    "\n",
    "In the next layer, we will have:\n",
    "\n",
    "$$a_1(2) = w_{11} (1) a_1(1) + w_{21} (1) a_2(1) + w_{31}(1) a_3(1)$$\n",
    "\n",
    "And so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee96e4",
   "metadata": {},
   "source": [
    "These lot of indices and subscripts may sound daunting at first. But the bottomline is the fact that all of them boil down to a linear function. Even if we have 1000 layers network, its just a recursive combination of linear functions. In fact, if we really consider it, the equations would be something like:\n",
    "\n",
    "$$a_1 (1000) = w_{11} a_1(999) + w_{21} a_2 (999) + w_{31} a_3 (999)$$\n",
    "\n",
    "Which itself is a linear combination of linear functions (you can either spend whole week in checking it for each layer or just use induction from the first two layers to reach this conclusion immediately). So, in the end, the network is unable to learn any non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae15830b",
   "metadata": {},
   "source": [
    "## Non-Linearity\n",
    "\n",
    "Hearing this term in the context of activation functions won't be a new thing for many of us. But my experience tells that students still don't get it (I remember 3 professors arguing over it a few years ago; hint: all were \"right\"). Lets realize it with a couple of classification problems.\n",
    "\n",
    "![](linearly_separable.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdb2f7",
   "metadata": {},
   "source": [
    "The classification problem above is straightforward: draw a line between the two clusters and its classified. Good!\n",
    "But most of the real life data examples are much complex than that. Like this:\n",
    "\n",
    "![](linearly_inseparable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24633388",
   "metadata": {},
   "source": [
    "It's obvious that any straight line (unless its a cop's rod) [can't classify the two](https://playground.tensorflow.org/#activation=linear&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.84658&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). So, we need to think beyond the linear functions. And for that, we introduce some [non-linear functions](https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.84658&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false), like Sigmoid, ReLU, etc. \n",
    "\n",
    "That's it. Have a good day!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
